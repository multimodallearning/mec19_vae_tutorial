{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import wget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# settings for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# misc\n",
    "def parameter_count(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        channels_in = 3\n",
    "        channels_out = 32\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers.append(nn.Conv2d(channels_in, channels_out, 3, stride=1, padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(channels_out))\n",
    "            layers.append(nn.ReLU())\n",
    "            channels_in = channels_out\n",
    "            \n",
    "            if(i%2==1):\n",
    "                channels_out *=2\n",
    "                layers.append(nn.MaxPool2d(2))\n",
    "\n",
    "        layers.append(nn.Conv2d(channels_in, channels_in, 1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(channels_in))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(channels_in, 2, 1))\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train/test dataset (this may take a minute or two)\n",
    "if not os.path.isfile('tcia_pancreas_data.pth'):\n",
    "    print('downloading tcia pancreas data from https://cloud.imi.uni-luebeck.de/s/Z7LWqcbopfwzmKn/download')\n",
    "    wget.download('https://cloud.imi.uni-luebeck.de/s/Z7LWqcbopfwzmKn/download')\n",
    "    \n",
    "imgs = torch.clamp(torch.load('tcia_pancreas_data.pth').float()/128-1, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0: Familiarize yourself with the data set and visualize some CT image slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise some data\n",
    "## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Build a variational autoencoder architecture and train with KLD loss\n",
    "\n",
    "1. Create an encoder that takes Bx3x256x256 input images and produces two 512 dim. latent vectors (μ and σ). Use nine blocks of Conv2d > BNorm > LeakyReLU with kernel size 3x3 and an increasing number of filter channels from 16 to 64. Use five stride=2 convolutions (every other layer) and padding=1. Add fully-connected layers with 1024 and then 512 channels (both implemented as Conv2d) the first one requires a kernel-size of 8x8.\n",
    "2. Implement the decoder, which takes a Bx512x1x1 input and should generate a full-sized (Bx3x256x256) output image. Start with one fully-connected 1x1 Conv2d layer (512>1024 channels) followed by a ConvTranspose2d with kernel=8, channel-out=64 and no padding. Use then again blocks of Conv2d of kernel size 3x3, but alternate them with ConvTranspose2d of size 4x4 with stride 2. All these Conv-Layers should have appropriate padding and BNorm > LeakyReLU. Finish the architecture with a Conv2d with 3 output-channels and a tanh.\n",
    "3. Build a VAE with your encoder-decoder architecture, you can directly follow the example from https://github.com/pytorch/examples/blob/master/vae/main.py.\n",
    "Note: mu and log_var have to be returned after the forward path for loss calculations.\n",
    "4.  Train this network for 160 epochs with batch size = 32, learning rate = 0.0025. Use the L1loss for the reconstruction of images and the Kullback-Leibler divergence (KLD) as shown in the example. Visualise some reconstructed examples after every 15 epoch (given a training image as input) as well as synthetically created examples, you can simply pass a torch.randn(B, 512,1,1) tensor to the decode function of your VAE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of vae model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = ## TODO ##\n",
    "        self.encoder_mu = nn.Conv2d(1024, 512, 1)\n",
    "        self.encoder_logvar = nn.Conv2d(1024, 512, 1)\n",
    "        \n",
    "        self.decoder = ## TODO ##\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.encoder_mu(x), self.encoder_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "print('parameter count:', parameter_count(VAE()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vae with perceptual loss\n",
    "\n",
    "# parameters\n",
    "batch_size = 32\n",
    "init_lr = 0.0025\n",
    "num_epochs = 160\n",
    "\n",
    "# data\n",
    "class TCIAPancreasDataset(Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.imgs = imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        return img\n",
    "    \n",
    "data_set = TCIAPancreasDataset(imgs)\n",
    "data_loader = DataLoader(data_set, batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# model\n",
    "vae = VAE()\n",
    "vae.cuda()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=init_lr)\n",
    "\n",
    "# for num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    ## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement a perceptual loss and add this to your VAE training\n",
    "\n",
    "A pre-trained fully-convolutional network which roughly follows the VGG architecture that has been trained for CT segmentation is provided for this task. \n",
    "\n",
    "1. Implement a forward hook function to extract perceptual features from after the ReLUs in layers 2, 5 and 9. You can use the following snippet to store the output during a forward path: \n",
    "\n",
    "```\n",
    "def get_output():\n",
    "    def hook(model, input, output):\n",
    "        model.output = output\n",
    "    return hook\n",
    "    \n",
    "```\n",
    "\n",
    "2. Next you need to call the method register_forward_hook for layers 2, 5 and 9 with get_output as function argument. After a forward path of your ground truth (training) image through the vgg_model you can copy the output tensors to a list of tensors. Repeat the same procedure after passing the reconstructed image through your network.  Use the L1Loss between those two feature tensors for all three layers as additional perceptual loss and retrain the network from scratch. Visualise the outputs to see whether there are any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vae with perceptual loss\n",
    "\n",
    "# parameters\n",
    "batch_size = 32\n",
    "init_lr = 0.0025\n",
    "num_epochs = 160\n",
    "\n",
    "# data\n",
    "class TCIAPancreasDataset(Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.imgs = imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        return img\n",
    "    \n",
    "data_set = TCIAPancreasDataset(imgs)\n",
    "data_loader = DataLoader(data_set, batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# model\n",
    "vae = VAE()\n",
    "vae.cuda()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=init_lr)\n",
    "\n",
    "# criterion\n",
    "vgg = VGG()\n",
    "\n",
    "if not os.path.isfile('tcia_pancreas_vgg.pth'):\n",
    "    print('downloading tcia pancreas vgg model from https://cloud.imi.uni-luebeck.de/s/xJ8edbQ3mmZK7Tg/download')\n",
    "    wget.download('https://cloud.imi.uni-luebeck.de/s/xJ8edbQ3mmZK7Tg/download')                       \n",
    "vgg_state_dict = torch.load('tcia_pancreas_vgg.pth')\n",
    "vgg.features.load_state_dict(vgg_state_dict, False)\n",
    "vgg.cuda()\n",
    "vgg.eval()\n",
    "\n",
    "def get_output():\n",
    "    def hook(model, input, output):\n",
    "        model.output = output\n",
    "    return hook\n",
    "\n",
    "layer = [2,5,9]\n",
    "for i in layer:\n",
    "    vgg.features[i].register_forward_hook(get_output())\n",
    "\n",
    "l1_loss = nn.L1Loss()    \n",
    "    \n",
    "def perceptual_loss(img_batch, recon):\n",
    "    with torch.no_grad():\n",
    "        img_batch_out = vgg(img_batch)\n",
    "    features_img_batch = []\n",
    "    for i in layer:\n",
    "        features_img_batch.append(vgg.features[i].output)\n",
    "        \n",
    "    recon_out = vgg(recon)\n",
    "    features_recon = []\n",
    "    for i in layer:\n",
    "        features_recon.append(vgg.features[i].output)\n",
    "    \n",
    "    loss = 0.0\n",
    "    for i in range(len(layer)):\n",
    "        loss += l1_loss(features_recon[i], features_img_batch[i])\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def kld_loss(mu, logvar):\n",
    "    return (-0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()))\n",
    "\n",
    "def criterion(img_batch, recon, mu, logvar):\n",
    "    return l1_loss(recon, img_batch) + kld_loss(mu, logvar) + perceptual_loss(img_batch, recon)\n",
    "\n",
    "# statistics\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# for num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    ## TODO ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Adress the checkerboard pattern in the synthetically generated images\n",
    "1. As described in the following blog article: https://distill.pub/2016/deconv-checkerboard/ the checkerboard could be reduced by replacing transpose convolutions with bilinear upsampling. Explore how and whether you can visually improve your synthetic image generation like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of vae model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.encoder = ## TODO ##\n",
    "        self.encoder_mu = nn.Conv2d(1024, 512, 1)\n",
    "        self.encoder_logvar = nn.Conv2d(1024, 512, 1)\n",
    "        \n",
    "        self.decoder = ## TODO ##\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.encoder_mu(x), self.encoder_logvar(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "print('parameter count:', parameter_count(VAE()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train vae with perceptual loss and adress checkerboard pattern\n",
    "\n",
    "# parameters\n",
    "batch_size = 32\n",
    "init_lr = 0.0025\n",
    "num_epochs = 160\n",
    "\n",
    "# data\n",
    "class TCIAPancreasDataset(Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.imgs = imgs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        return img\n",
    "    \n",
    "data_set = TCIAPancreasDataset(imgs)\n",
    "data_loader = DataLoader(data_set, batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "\n",
    "# model\n",
    "vae = VAE()\n",
    "vae.cuda()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(vae.parameters(), lr=init_lr)\n",
    "\n",
    "# for num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    ## TODO ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
